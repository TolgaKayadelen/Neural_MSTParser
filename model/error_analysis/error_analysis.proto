// Protocol buffer specification for error analysis of a model.

syntax = "proto2";

import "model/tags_and_labels_enum.proto";
import "model/evaluation.proto";

package model.error_analysis;

message ModelErrorAnalysis {
  optional Metadata metadata = 1;
  optional ModelEval model_eval = 2;
  optional UnlabeledAttachmentAnalysis unlabeled_attachment = 3;
  optional LabeledAttachmentAnalysis labeled_attachment = 4;
  repeated SentenceEval sentence_evals = 5;
}
message Metadata {
  // the name of the model which we analyze.
  optional string model_name = 1;
  // the data with which the model was trained.
  optional string trained_on = 2;
  // the number of training epochs.
  optional int epochs = 3;
  // number of features in the model.
  optional int feature_count = 4
  // arc accuracy of the model.
  optional float arc_accuracy = 5;
  // the data which we are evaluating the model with. This is the data in
  // which the sentences are parsed with this model.
  optional string eval_data = 6;
  // the gold data which we are evaluating the model against. This is the data
  // which has ground truth annotations.
  optional string gold_data = 7;
  // the language of the model.
  optional Language language = 8;
}

message ModelEval {
	//evaluation_pb2.Evaluation()
  optional Evaluation = 1;
}

message UnlabeledAttachmentAnalysis {
  repeated PosTagAttachmentAnalysis analysis = 1;
}

message LabeledAttachmentAnalysis {
  repeated PosTagLabelAnalysis analysis = 1;
}

// Sentence based unlabeled attachment and labeled attachment scores.
message SentenceEval {
  optional string sentence = 1;
  optional float uas = 2;
  optional float las = 3;
}

// The postag attachment analysis provides an evaluation of how accurately
// we can predict the correct head for each of the part of speech categories
// in the data, and with which part of speech do we mostly confuse the heads
// with. This analysis will give us an overall picture of how hard the
// attachment problem is for different part of speech categories.
// Example:
// Tag: NOUN     # doing unlabeled attachment analysis for the NOUN pos.
// total: 5      # there are 5 instances of nouns in the data.
// times_head_correct: 3 # in 3 instances I predicted the head correct for NOUNs
// times_head_wrong: 2
// attachment_score: 0.6 # i.e. 3/5
// confusion {  # the confusion analysis.
// count: 1 # this confusion type was observed once.
// correct_head_pos: VERB # the correct head pos was verb.
// predicted_head_pos: ROOT # but I predicted the head as ROOT.
// Examples {
//    Sentence: "the sentence where this confusion is observed."
//    Indexes: [[1,2,3]]
//  }
// }
message PosTagAttachmentAnalysis {
  optional Tag.Value tag = 1;
  float total = 2;
  float times_head_correct = 3;
  float times_head_wrong = 4;
  float attachment_score = 5;
  repeated AttachmentConfusion confusion = 6;
}

message AttachmentConfusion {
  int count = 1;
  optional Tag.Value correct_head_pos = 1;
  optional Tag.Value predicted_head_pos = 2;
  repeated Example examples = 4;
}

enum Language {
	UNKNOWN_LANGUAGE = 0;
	TURKISH = 1;
	ENGLISH = 2;
}
